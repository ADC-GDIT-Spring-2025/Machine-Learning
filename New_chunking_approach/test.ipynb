{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = \"C:\\\\Users\\\\ncvn\\\\.cache\\\\kagglehub\\\\datasets\\\\wcukierski\\\\enron-email-dataset\\\\versions\\\\2\\\\emails.csv\"\n",
    "# df = pd.read_csv(\"C:\\\\Users\\\\ncvn\\\\.cache\\\\kagglehub\\\\datasets\\\\wcukierski\\\\enron-email-dataset\\\\versions\\\\2\\\\emails.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1000 emails\n",
      "Saving processed data to filtered.csv...\n",
      "Saved processed data to filtered.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "# loading spaCy's small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def parse_emails(file_path):\n",
    "    \"\"\"\n",
    "    Purpose: Parse a raw email file and extract email messages.\n",
    "    \"\"\"\n",
    "    emails = []\n",
    "    current_email = {}\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            # checking for email headers\n",
    "            if line.startswith(('Subject:', 'From:', 'To:', 'cc:', 'bcc:', 'Mime-Version:', 'Content-Type:', 'X-From:', 'X-To:', 'X-cc:', 'X-bcc:', 'X-Folder:', 'X-Origin:', 'X-FileName:')):\n",
    "                key, value = line.split(':', 1)\n",
    "                current_email[key.strip()] = value.strip()\n",
    "\n",
    "            # checking for the start of the email body\n",
    "            elif line == '' and 'body' not in current_email:\n",
    "                current_email['body'] = ''\n",
    "\n",
    "            # appending lines to the email body\n",
    "            elif 'body' in current_email:\n",
    "                current_email['body'] += line + '\\n'\n",
    "\n",
    "            # checking for the end of an email message\n",
    "            if line.startswith('-----'):\n",
    "                emails.append(current_email)\n",
    "                current_email = {}\n",
    "\n",
    "        # appending the last email if the file doesn't end with a separator\n",
    "        if current_email:\n",
    "            emails.append(current_email)\n",
    "\n",
    "    return emails\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Purpose: Clean and preprocess text.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove email forward markers and headers\n",
    "    text = re.sub(r'---+ ?forwarded by.+?---+', ' ', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    text = re.sub(r'---+ ?original message.+?---+', ' ', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    text = re.sub(r'---+ ?forwarded message.+?---+', ' ', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    # Remove email headers in the body\n",
    "    text = re.sub(r'from:.*?(?=\\n\\n|\\n\\w)', ' ', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    text = re.sub(r'sent:.*?(?=\\n\\n|\\n\\w)', ' ', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    text = re.sub(r'to:.*?(?=\\n\\n|\\n\\w)', ' ', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    text = re.sub(r'subject:.*?(?=\\n\\n|\\n\\w)', ' ', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    text = re.sub(r'cc:.*?(?=\\n\\n|\\n\\w)', ' ', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    text = re.sub(r'bcc:.*?(?=\\n\\n|\\n\\w)', ' ', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    # Remove message IDs and timestamps\n",
    "    text = re.sub(r'message-id:.*', ' ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'date:.*', ' ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'content-transfer-encoding:.*', ' ', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', text)\n",
    "\n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', ' ', text)\n",
    "\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s.,!?-]', ' ', text)\n",
    "\n",
    "    # Remove excess whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Purpose: Tokenize text into words and apply lemmatization using spaCy.\n",
    "    \"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return []\n",
    "\n",
    "    # processing text with spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # extracting tokens and apply lemmatization\n",
    "    tokens = [token.lemma_ for token in doc if token.is_alpha or token.is_punct]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def preprocess_data(input_path, output_path, max_rows=None):\n",
    "    \"\"\"\n",
    "    Preprocess the Enron email dataset.\n",
    "    \"\"\"\n",
    "    # parse the raw email file\n",
    "    emails = parse_emails(input_path)\n",
    "\n",
    "    # convert the list of emails to a DataFrame\n",
    "    df = pd.DataFrame(emails)\n",
    "\n",
    "    # handle missing data\n",
    "    df['Subject'] = df['Subject'].fillna('')\n",
    "    df['body'] = df['body'].fillna('')\n",
    "\n",
    "    # limit the number of rows if max_rows is specified\n",
    "    if max_rows:\n",
    "        df = df.head(max_rows)\n",
    "\n",
    "    print(f\"Processing {len(df)} emails\")\n",
    "\n",
    "    # cleaning and tokenizing the subject\n",
    "    df['subject_clean'] = df['Subject'].apply(clean_text)\n",
    "    df['subject_tokens'] = df['subject_clean'].apply(tokenize_text)\n",
    "\n",
    "    # cleaning and tokenizing the email body\n",
    "    df['body_clean'] = df['body'].apply(clean_text)\n",
    "    df['body_tokens'] = df['body_clean'].apply(tokenize_text)\n",
    "\n",
    "    # selecting all the required columns\n",
    "    df = df[['From', 'To', 'subject_clean', 'body_clean', 'subject_tokens', 'body_tokens']]\n",
    "\n",
    "    # save the processed data\n",
    "    print(f\"Saving processed data to {output_path}...\")\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved processed data to {output_path}\")\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    processed_df = preprocess_data(path, \"filtered.csv\", max_rows=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TheVirtualEnv",
   "language": "python",
   "name": "thevirtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
