{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\ncvn\\\\.cache\\\\kagglehub\\\\datasets\\\\wcukierski\\\\enron-email-dataset\\\\versions\\\\2\\\\emails.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df.head())  \n",
    "print(df.info()) \n",
    "print(df.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['file'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(df['message'][30000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(df['message'][30000]).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import enum\n",
    "# import networkx as nx\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# G = nx.DiGraph()\n",
    "# df2 = df.iloc[:10000]\n",
    "# senders = []\n",
    "# recipients = []\n",
    "# dict_sr = {}\n",
    "# for i, (_, row) in enumerate(df2.iterrows()):\n",
    "    \n",
    "#     split_msg = str(row[\"message\"]).split()\n",
    "#     # print(i,row[\"message\"])\n",
    "#     # print(split_msg)\n",
    "#     sender = split_msg[split_msg.index(\"From:\")+1]\n",
    "#     senders.append(sender)\n",
    "#     if sender not in dict_sr:\n",
    "#         dict_sr[sender] = []\n",
    "#     #receiver = split_msg[split_msg.index(\"To:\")+1]\n",
    "#     # print(sender)\n",
    "    \n",
    "#     try:\n",
    "#         for idx in range(split_msg.index(\"To:\")+1, split_msg.index(\"Subject:\")):\n",
    "#             dict_sr[sender].append(split_msg[idx])\n",
    "#     except:\n",
    "#         for idx in range(split_msg.index(\"X-To:\")+1, split_msg.index(\"Subject:\")):\n",
    "#             dict_sr[sender].append(split_msg[idx])\n",
    "    \n",
    "#     dict_sr[sender].sort()\n",
    "\n",
    "#     # print(recipients)\n",
    "#     # for recipient in recipients:\n",
    "#     #     G.add_edge(sender, recipient)\n",
    "\n",
    "# # plt.figure(figsize=(12, 8))\n",
    "# # nx.draw(G, with_labels=True, node_size=1000, font_size=8, edge_color=\"gray\")\n",
    "# # plt.title(\"Enron Sender-Recipient Email Network\")\n",
    "# # plt.show()\n",
    "# # print(senders,'\\n',recipients)\n",
    "# # print(len(senders),len(recipients),senders.count('phillip.allen@enron.com') / len(senders))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# embedding_model = HuggingFaceEmbeddings()\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}  # set True to compute cosine similarity\n",
    "model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")\n",
    "# Split email texts into chunks for better retrieval\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=350, chunk_overlap=50)\n",
    "email_texts = df[\"message\"].iloc[:10000].dropna().tolist()\n",
    "# email_chunks = text_splitter.split_text(\"\\n\".join(email_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(email_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "import pprint\n",
    "docslist = []\n",
    "for i, ec in enumerate(email_texts):\n",
    "    #pprint.pprint(ec.split())\n",
    "    \n",
    "    split_msg = ec.split()\n",
    "    msg_start = split_msg.index(\"X-FileName:\")\n",
    "    sender = split_msg[split_msg.index(\"From:\")+1]\n",
    "    recips = []\n",
    "    #print(f'EMAIL CHUNK {i}\\n\\n',ec.split()[msg_start+3:])\n",
    "    try:\n",
    "        for idx in range(split_msg.index(\"To:\")+1, split_msg.index(\"Subject:\")):\n",
    "            recips.append(split_msg[idx])\n",
    "    except:\n",
    "        for idx in range(split_msg.index(\"X-To:\")+1, split_msg.index(\"Subject:\")):\n",
    "            recips.append(split_msg[idx])\n",
    "\n",
    "    date_idx = split_msg.index(\"Date:\")+1\n",
    "    date_idx_end = date_idx + 6\n",
    "    full_content = \" \".join(split_msg[msg_start:])\n",
    "    #docslist.append(Document(page_content=full_content, metadata={\"sender\":sender,\"recipient\":\" \".join(recips),\"date\":\" \".join(split_msg[date_idx : date_idx_end])}))\n",
    "    prefix = {\"sender\":sender,\"recipient\":\" \".join(recips),\"date\":\" \".join(split_msg[date_idx : date_idx_end])}\n",
    "    #print(list(zip(prefix.keys(),prefix.values())))\n",
    "    final_list = text_splitter.split_text(full_content)\n",
    "    #print(list(zip(prefix.keys(),prefix.values()))+final_list)\n",
    "    for f in final_list:\n",
    "        docslist.append(str(list(zip(prefix.keys(),prefix.values())))+f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_doc_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in docslist:\n",
    "    print(d)\n",
    "    final_doc_list.append(Document(page_content=d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(final_doc_list[:300], model)\n",
    "db.save_local(\"email_faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docslist = final_doc_list[301:602]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 20\n",
    "# for i in range(0, len(docslist), batch_size):\n",
    "#     batch_docs = docslist[i : i + batch_size]  \n",
    "#     await db.aadd_documents(batch_docs)  \n",
    "#     print(f\"Batch {i // batch_size} added ({len(batch_docs)} docs).\")\n",
    "\n",
    "import asyncio\n",
    "import concurrent.futures\n",
    "from langchain_community.vectorstores import FAISS\n",
    "def process_batch_sync(db, batch_docs, batch_id):\n",
    "    loop = asyncio.new_event_loop()\n",
    "    asyncio.set_event_loop(loop)\n",
    "    loop.run_until_complete(db.aadd_documents(batch_docs))\n",
    "    loop.close()\n",
    "    print(f\"âœ… Batch {batch_id} added ({len(batch_docs)} docs).\")\n",
    "\n",
    "async def batch_insert(db, docslist, batch_size=20, num_workers=6):\n",
    "    loop = asyncio.get_running_loop()\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        tasks = []\n",
    "        \n",
    "        for i in range(0, len(docslist), batch_size):\n",
    "            batch_docs = docslist[i : i + batch_size]\n",
    "            batch_id = i // batch_size\n",
    "            tasks.append(loop.run_in_executor(executor, process_batch_sync, db, batch_docs, batch_id))\n",
    "        \n",
    "        await asyncio.gather(*tasks) \n",
    "\n",
    "# Run the optimized batch insertion\n",
    "await batch_insert(db, docslist, batch_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.save_local(\"email_faiss\")\n",
    "print(\"FAISS index updated and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.load_local(\"email_faiss\",model,allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from initialize_groq import init_groq\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "            Answer question only provided the context. Give a detailed answer IN minimum 5 sentences!\n",
    "            SAY I DONT KNOW IF CONTEXT IS NOT ENOUGH. DONT MAKE UP ANSWERS. BUT YOU ARE FREE TO INFER/SUGGEST.\n",
    "            {context}\n",
    "\n",
    "            Here is question:\n",
    "            {input}\n",
    "        \"\"\"\n",
    ")\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={'k':20, 'search_type':'mmr','lambda_mult':0.2})\n",
    "\n",
    "_, llm = init_groq(model_name=\"llama-3.3-70b-versatile\")\n",
    "import random\n",
    "from initialize_groq import api_keys\n",
    "llm.groq_api_key = random.choice(api_keys)\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# Step 1: Retrieve Top-K Similar Documents (Initial Broad Search)\n",
    "retriever_topk = db.as_retriever(search_kwargs={'k': 20, 'search_type': 'similarity'})  # Retrieve more docs first\n",
    "\n",
    "# Step 2: Apply MMR for Diversity (Reduce Redundant Docs)\n",
    "retriever_mmr = db.as_retriever(search_kwargs={'k': 20, 'search_type': 'mmr', 'lambda_mult': 0.2})  \n",
    "\n",
    "# # Step 3: Apply Similarity Threshold to Remove Low-Relevance Docs\n",
    "retriever_threshold = db.as_retriever(search_kwargs={'k': 20, 'search_type': 'similarity_score_threshold', 'score_threshold': -1})\n",
    "\n",
    "# Create the Hybrid Retrieval Pipeline\n",
    "retrieval_chain_topk = create_retrieval_chain(retriever_topk, document_chain)  # Initial broad search\n",
    "retrieval_chain_mmr = create_retrieval_chain(retriever_mmr, document_chain)    # Apply MMR re-ranking\n",
    "retrieval_chain_threshold = create_retrieval_chain(retriever_threshold, document_chain)  # Final filtering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in docslist:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "query = \"give me 5 emails involving buck buckner of honeywell\"\n",
    "pprint.pprint(retrieval_chain_topk.invoke({\"input\":query}))\n",
    "\n",
    "pprint.pprint(retrieval_chain_mmr.invoke({\"input\":query}))\n",
    "\n",
    "# pprint.pprint(retrieval_chain_threshold.invoke({\"input\":query}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docslist:\n",
    "    print(len(doc.page_content.split()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "# Ensure retrieval_chain is correctly defined before calling this tool\n",
    "@tool\n",
    "def ragtool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    This is a retrieval-augmented generation (RAG) tool that queries a vector store \n",
    "    containing Enron emails.\n",
    "    \n",
    "    Parameters:\n",
    "    query (str): The input query for retrieval.\n",
    "    \n",
    "    Returns:\n",
    "    str: The retrieved answer from the vector store.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        answer = retrieval_chain_mmr.invoke({\"input\": query})['answer']\n",
    "        return f\"Here is the ANSWER. STOP AND WAIT FOR USER QUERY. \\n```{answer}```\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: Failed to retrieve answer. Details: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "\n",
    "\n",
    "toolnode = ToolNode([ragtool])\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    state[\"messages\"]\n",
    "    messages = state[\"messages\"]\n",
    "    #print(messages)\n",
    "    llm.groq_api_key = random.choice(api_keys)\n",
    "    llm_with_tool = llm.bind_tools([ragtool])\n",
    "    response = llm_with_tool.invoke(messages)\n",
    "    \n",
    "    \n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "from langgraph.graph import END\n",
    "def router_function(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "memory = MemorySaver()\n",
    "workflow = StateGraph(MessagesState)    \n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(toolnode)\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    router_function,\n",
    "    {\n",
    "       \"tools\": \"tools\",\n",
    "       END: END,\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_png\n",
    "display_png(app.get_graph().draw_mermaid_png(),raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "while True:\n",
    "    theinput = input(\"Enter something: \")\n",
    "    if 'exit' in theinput:\n",
    "        break\n",
    "    inp = {\"messages\":[theinput]}\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": 1}}\n",
    "    events = app.stream(inp, config=config, stream_mode=\"values\")\n",
    "\n",
    "    for event in events:\n",
    "        event[\"messages\"][-1].pretty_print()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Literal, List\n",
    "# from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "# from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "# from langgraph.checkpoint.memory import MemorySaver\n",
    "# from langchain.memory import ChatMessageHistory\n",
    "# from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\n",
    "# from langgraph.prebuilt import ToolNode\n",
    "# from langchain_core.documents import Document\n",
    "# from langchain_core.tools import tool\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# # === Define Custom Tools ===\n",
    "\n",
    "\n",
    "# @tool\n",
    "# def filter_emails_by_keyword(emails: List[Document], keywords: List[str]) -> List[Document]:\n",
    "#     \"\"\"Filter emails that contain the given keywords in the content or metadata.\"\"\"\n",
    "#     def filter_email(email):\n",
    "#         content = email.page_content.lower()\n",
    "#         metadata = \" \".join(str(val).lower() for val in email.metadata.values())\n",
    "#         return any(keyword.lower() in content or keyword.lower() in metadata for keyword in keywords)\n",
    "    \n",
    "#     with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#         filtered_emails = list(filter(None, executor.map(lambda e: e if filter_email(e) else None, emails)))\n",
    "\n",
    "#     return filtered_emails\n",
    "\n",
    "# @tool\n",
    "# def filter_emails_by_metadata(\n",
    "#     emails: List[Document], sender: str = None, recipient: str = None, date: str = None\n",
    "# ) -> List[Document]:\n",
    "#     \"\"\"Filter emails by metadata fields like sender, recipient, or date.\"\"\"\n",
    "#     def filter_email(email):\n",
    "#         if sender and email.metadata.get(\"sender\", \"\").lower() != sender.lower():\n",
    "#             return None\n",
    "#         if recipient and recipient.lower() not in email.metadata.get(\"recipient\", \"\").lower():\n",
    "#             return None\n",
    "#         if date and date not in email.metadata.get(\"date\", \"\"):\n",
    "#             return None\n",
    "#         return email\n",
    "    \n",
    "#     with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#         filtered_emails = list(filter(None, executor.map(filter_email, emails)))\n",
    "    \n",
    "#     return filtered_emails\n",
    "\n",
    "# @tool\n",
    "# def summarize_emails(emails: List[Document]) -> List[Document]:\n",
    "#     \"\"\"Summarize emails before adding them to FAISS.\"\"\"\n",
    "#     _,llm = init_groq(model_name=\"llama-3.3-70b-versatile\")\n",
    "#     prompt = ChatPromptTemplate.from_template(\"Summarize the following email:\\n{email}\")\n",
    "#     summarized_docs = []\n",
    "#     for email in emails:\n",
    "#         chain = prompt | llm\n",
    "#         summary = chain.invoke(email.page_content)\n",
    "#         summarized_docs.append(Document(page_content=summary.content, metadata=email.metadata))\n",
    "#     return summarized_docs\n",
    "\n",
    "# toolnode = ToolNode([ragtool, filter_emails_by_keyword, filter_emails_by_metadata, summarize_emails])\n",
    "# llm_with_tool = llm.bind_tools([ragtool, filter_emails_by_keyword, filter_emails_by_metadata, summarize_emails])\n",
    "\n",
    "# # === Define Model Function ===\n",
    "# def call_model(state: MessagesState):\n",
    "#     \"\"\"Modify agent behavior to apply filtering and summarization before RAG.\"\"\"\n",
    "#     messages = state[\"messages\"]\n",
    "#     query = messages[-1]\n",
    "\n",
    "#     # Step 1: Apply Keyword Filtering\n",
    "#     filtered_emails = filter_emails_by_keyword.invoke({\"emails\": docslist, \"keywords\": [query.content]})\n",
    "\n",
    "#     # Step 2: Apply Metadata Filtering\n",
    "#     filtered_emails = filter_emails_by_metadata.invoke(\n",
    "#         {\"emails\": filtered_emails, \"sender\": \"\", \"recipient\": \"\", \"date\": \"\"}\n",
    "#     )\n",
    "\n",
    "#     # Step 3: Summarize Emails if Needed\n",
    "#     summarized_emails = summarize_emails.invoke({\"emails\": filtered_emails})\n",
    "\n",
    "#     # Step 4: Run RAG Tool on Filtered Emails\n",
    "#     state[\"messages\"].append(\"\\n\")\n",
    "#     response = llm_with_tool.invoke([summarized_emails])\n",
    "\n",
    "#     return {\"messages\": [response]}\n",
    "\n",
    "# # === Define Router Function ===\n",
    "# def router_function(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "#     messages = state[\"messages\"]\n",
    "#     last_message = messages[-1]\n",
    "#     if last_message.tool_calls:\n",
    "#         return \"tools\"\n",
    "#     return END\n",
    "\n",
    "# # === Build LangGraph Workflow ===\n",
    "# memory = MemorySaver()\n",
    "# workflow = StateGraph(MessagesState)\n",
    "# workflow.add_node(\"agent\", call_model)\n",
    "# workflow.add_node(toolnode)\n",
    "# workflow.add_edge(START, \"agent\")\n",
    "# workflow.add_conditional_edges(\n",
    "#     \"agent\",\n",
    "#     router_function,\n",
    "#     {\n",
    "#         \"tools\": \"tools\",\n",
    "#         END: END,\n",
    "#     },\n",
    "# )\n",
    "# workflow.add_edge(\"tools\", \"agent\")\n",
    "# app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# while True:\n",
    "#     theinput = input(\"Enter something: \")\n",
    "#     if 'exit' in theinput:\n",
    "#         break\n",
    "#     inp = {\"messages\":[theinput]}\n",
    "\n",
    "#     config = {\"configurable\": {\"thread_id\": 1}}\n",
    "#     events = app.stream(inp, config=config, stream_mode=\"values\")\n",
    "\n",
    "#     for event in events:\n",
    "#         event[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    theinput = input(\"Enter something: \")\n",
    "    if 'exit' in theinput:\n",
    "        break\n",
    "    inp = {\"messages\":[theinput]}\n",
    "\n",
    "    config = {\"configurable\": {\"thread_id\": 1}}\n",
    "    events = app.stream(inp, config=config, stream_mode=\"values\")\n",
    "\n",
    "    for event in events:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TheVirtualEnv",
   "language": "python",
   "name": "thevirtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
