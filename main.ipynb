{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acba0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "import torch\n",
    "from gliner import GLiNER\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "import re\n",
    "from pprint import pprint\n",
    "import random\n",
    "from utilities import *\n",
    "from chunking import EnhancedSemanticChunker\n",
    "from langchain_qdrant import Qdrant\n",
    "from initialize_groq import init_groq\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "df = pd.read_csv(\"data/emails.csv\")\n",
    "idx = 1000\n",
    "msg = df['message'][idx]\n",
    "VECTOR_DB_NAME = \"emails_e5_qdrant\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fdc5ec",
   "metadata": {},
   "source": [
    "THIS IS TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3f509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(msg)\n",
    "print(clean_text(msg))\n",
    "print('cuda available?',torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e1e0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing Microsoft E5 model\n",
    "model_kwargs = {'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}  # does l2 norm for the cos sim\n",
    "modelemb = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/e5-base-v2\", \n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc70b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing GLiNER model\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gliner_model = GLiNER.from_pretrained(\"urchade/gliner_medium-v2.1\")\n",
    "gliner_model.config.max_len = 512\n",
    "gliner_model.to(DEVICE)\n",
    "\n",
    "# model entity labels configuration\n",
    "labels = [\"date\", \"location\", \"person\", \"action\", \"finance\", \"legal\", \"event\", \"product\", \"organization\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a668ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing EnhancedSemanticChunker\n",
    "enhanced_chunker = EnhancedSemanticChunker(\n",
    "    embeddings=modelemb,\n",
    "    breakpoint_threshold_type=\"percentile\",\n",
    "    breakpoint_threshold_amount=50,  \n",
    "    min_chunk_size=5,\n",
    "    overlap_sentences=2,  \n",
    "    gliner_model=gliner_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d458fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata, split_msg = extract_email_metadata(msg, idx)\n",
    "msg_start = split_msg.index(\"X-FileName:\")\n",
    "full_content = clean_text(\" \".join(split_msg[msg_start + 2:]))\n",
    "# print(full_content)\n",
    "# Create document with the enhanced chunker\n",
    "documents = enhanced_chunker.create_documents(\n",
    "    texts=[full_content],\n",
    "    metadatas=[metadata]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0a358f",
   "metadata": {},
   "source": [
    "THIS IS TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24654ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of chunks after deduplication: {len(documents)}\")\n",
    "\n",
    "# Now use 'deduplicated_documents' for further processing (indexing, etc.)\n",
    "for i, doc in enumerate(documents):  \n",
    "    print(f\"\\n--- Unique Chunk {i+1}/{len(documents)} ---\")\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d812c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Qdrant.from_existing_collection(modelemb, \"qdrant_db\", VECTOR_DB_NAME)\n",
    "model = modelemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d06b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "            Answer question based on EMAIL METADATA AND CONTENT provided. CITE YOUR SOURCES.\n",
    "            {context}\n",
    "\n",
    "            Here is question:\n",
    "            {input}\n",
    "        \"\"\"\n",
    ")\n",
    "\n",
    "document_prompt = PromptTemplate.from_template(\n",
    "    \"METADATA: Source: {sender}\\nDate: {date}\\n Recipients: {recipient}\\nSubject: {subject}\\nEntities: {entities}\\n\\nContent: {page_content}\"\n",
    ")\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={'k':20, 'search_type':'mmr','lambda_mult':0.2})\n",
    "\n",
    "_, llm, groqllm = init_groq(model_name=\"llama-3.3-70b-versatile\")\n",
    "import random\n",
    "document_chain = create_stuff_documents_chain(llm, prompt=prompt, document_prompt=document_prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# Retrieve Top-K Similar Documents (Initial Broad Search)\n",
    "# retriever_topk = db.as_retriever(search_kwargs={'k': 20,'fetch_k' : 100, 'search_type': 'similarity_s core_threshold','score_threshold':0.75})  # Retrieve more docs first\n",
    "retriever_topk = db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'score_threshold':0.65,'k':10})\n",
    "# MMR for Diversity (Reduce Redundant Docs)\n",
    "retriever_mmr = db.as_retriever(search_type=\"mmr\", search_kwargs={'k':10,'lambda_mult': 1})  \n",
    "\n",
    "# Create the Hybrid Retrieval Pipeline\n",
    "retrieval_chain_topk = create_retrieval_chain(retriever_topk, document_chain)  # Initial broad search\n",
    "retrieval_chain_mmr = create_retrieval_chain(retriever_mmr, document_chain)    # Apply MMR re-ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cc5dd0",
   "metadata": {},
   "source": [
    "THIS IS TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d4c2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"query: is MSEB an indian company? 402 crore amount? its relation to enron?\"\n",
    "# pprint.pprint(retrieval_chain_topk.invoke({\"input\":query}))\n",
    "pprint.pprint(retrieval_chain_mmr.invoke({\"input\":query}))\n",
    "\n",
    "test_questions = [\n",
    "    \"What does randy need to send a schedule of?\",\n",
    "    \"What are some of randy's action items?\",\n",
    "    \"What is Philip's proposal focused on, and can you provided details about the proposal?\",\n",
    "    \"Can you provide me more detail about the microturbine power generation deal?\",\n",
    "    \"What needs to be faxed?\",\n",
    "    \"Are there hints of a scandal in the emails?\",\n",
    "    \"What did jeffrey skilling tell john arnold\"\n",
    "]\n",
    "for text in test_questions:\n",
    "    # Define query\n",
    "    query = \"query: \" + text\n",
    "    pprint.pprint(retrieval_chain_mmr.invoke({\"input\":query}))\n",
    "\n",
    "test_questions = [\n",
    "    \"query: What does randy need to send a schedule of?\",\n",
    "    \"query: What are some of randy's action items?\",\n",
    "    \"query: What is Philip's proposal focused on, and can you provided details about the proposal?\",\n",
    "    \"query: Can you provide me more detail about the microturbine power generation deal?\",\n",
    "    \"query: What needs to be faxed?\"\n",
    "]\n",
    "for text in test_questions:\n",
    "    print(\"=========================================================\")\n",
    "    query = \"query: \" + text\n",
    "    query_embedding = np.array(model.embed_query(query))\n",
    "    # query_embedding = l2_normalize(query_embedding)  \n",
    "    topk_results = db.similarity_search_with_score_by_vector(\n",
    "        embedding=query_embedding.tolist(),  # List[float]\n",
    "        k=5\n",
    "    )\n",
    "\n",
    "    mmr_results = db.max_marginal_relevance_search_with_score_by_vector(\n",
    "        embedding=query_embedding.tolist(),  # List[float]\n",
    "        k=5,\n",
    "        lambda_mult=0.8         \n",
    "    )\n",
    "\n",
    "    # Sort by L2 distance (ascending: lower = more similar)\n",
    "    topk_sorted = sorted(topk_results, key=lambda x: x[1])\n",
    "    \n",
    "    mmr_sorted = sorted(mmr_results, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Display results with L2 distance and cosine similarity\n",
    "    for doc, mmr_score in mmr_sorted:\n",
    "        # docembedding = l2_normalize(np.array(modelemb.embed_documents([doc.page_content])))\n",
    "        # cos_sim = float(np.dot(query_embedding, docembedding.reshape(-1)))\n",
    "        pprint.pprint(f\"Document:\\n {doc.page_content} | MMR Score: {mmr_score:.4f}\")\n",
    "        \n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "    for doc, l2_score in topk_sorted:\n",
    "        # Convert L2 distance to cosine similarity (assuming L2-normalized)\n",
    "        cosine_sim = 1 - (l2_score ** 2) / 2\n",
    "        pprint.pprint(f\"Document: {doc.page_content[:100]} | L2 Distance: {l2_score:.4f} | Cosine Sim: {cosine_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7009d233",
   "metadata": {},
   "outputs": [],
   "source": [
    "toolnode = ToolNode([ragtool])\n",
    "\n",
    "memory = MemorySaver()\n",
    "workflow = StateGraph(MessagesState)    \n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(toolnode)\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    router_function,\n",
    "    {\n",
    "       \"tools\": \"tools\",\n",
    "       END: END,\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e0d77d",
   "metadata": {},
   "source": [
    "THIS IS TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5ec88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_png\n",
    "display_png(app.get_graph().draw_mermaid_png(),raw=True)\n",
    "\n",
    "import time\n",
    "while True:\n",
    "    theinput = input(\"Enter something: \")\n",
    "    if 'exit' in theinput:\n",
    "        break\n",
    "    inp = {\"messages\":[theinput]}\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": 1}}\n",
    "    events = app.stream(inp, config=config, stream_mode=\"values\")\n",
    "\n",
    "    for event in events:\n",
    "        event[\"messages\"][-1].pretty_print()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae10d48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the multi-query prompt template\n",
    "# This template instructs the LLM to generate multiple search queries from a single user question\n",
    "multi_template = \"\"\"You are an expert at querying search engines. You specialize in understanding natural language queries and generating multiple search\n",
    "queries that, taken together, would help provide a comprehensive answer to the user's question.\n",
    "\n",
    "Main Question: {question}\n",
    "\n",
    "Let's break this down. Generate 4 search queries for querying a knowledge store about emails. \n",
    "Make sure these queries use language that would appear in actual emails.\n",
    "Remember to keep them short, using keywords that would be found in emails.\n",
    "Keep them straightforward and distinct from each other.\n",
    "Formulate them from different angles to solve the main query.\n",
    "\n",
    "Return a bullet list with • at the start of each question:\n",
    "\n",
    "• query 1\n",
    "• query 2\n",
    "• etc.\n",
    "\"\"\"\n",
    "\n",
    "# Create a processor to generate multiple search queries from a single question\n",
    "multi_query_prompt = PromptTemplate.from_template(multi_template)\n",
    "multi_query_chain = multi_query_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfaf82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test queries for evaluating retrieval performance\n",
    "test_queries = [\n",
    "    \"What do we know about Skilling's involvement in Enron's financial reporting?\",\n",
    "    \"What are the main topics discussed in emails from Kenneth Lay?\",\n",
    "    \"How did Enron executives discuss the California energy crisis in their emails?\",\n",
    "    \"What discussions were happening about LJM partnerships in the months before Enron's collapse?\",\n",
    "    \"What was discussed about mark-to-market accounting in emails?\",\n",
    "    \"Who was responsible for overseeing Special Purpose Entities at Enron?\",\n",
    "    \"What communication happened regarding Raptor structures?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0c8ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage - process a test query with advanced retrieval\n",
    "for i in range(len(test_queries)):\n",
    "    result = run_multi_query(test_queries[i])\n",
    "    # Print the final answer\n",
    "    print(result[\"final_answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
