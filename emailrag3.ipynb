{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae08733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/emails.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea51c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print('cuda available?',torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "022c933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_DB_NAME = \"my_documents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "820aab76",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_core.documents import Document\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional\n",
    "from gliner import GLiNER\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "\n",
    "class EnhancedSemanticChunker(SemanticChunker):\n",
    "    \"\"\"Enhanced Semantic Chunker with sentence-level overlap and NER-aware chunking.\n",
    "\n",
    "    This chunker extends the basic SemanticChunker with advanced features:\n",
    "    1. Controllable sentence overlap between chunks for context continuity\n",
    "    2. NER-aware chunk boundaries using GLiNER to preserve entity mentions\n",
    "    3. Formatted entity metadata enrichment for better retrieval\n",
    "\n",
    "    The chunker works by:\n",
    "    - Finding semantically coherent boundaries using embeddings\n",
    "    - Detecting entities to prevent splitting in the middle of important entities\n",
    "    - Adding controlled overlap between chunks to maintain context\n",
    "    - Enriching chunks with entity information for better retrieval\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings: Any,\n",
    "        gliner_model: GLiNER,\n",
    "        breakpoint_threshold_type: str = \"percentile\",\n",
    "        breakpoint_threshold_amount: int = 95,\n",
    "        min_chunk_size: int = 5,\n",
    "        max_chunk_size: Optional[int] = None,\n",
    "        overlap_sentences: int = 1,\n",
    "    ):\n",
    "        \"\"\"Initialize the enhanced semantic chunker.\n",
    "\n",
    "        Args:\n",
    "            embeddings: The embeddings model to use for semantic similarity calculation.\n",
    "            gliner_model: A loaded GLiNER model instance for named entity recognition.\n",
    "            breakpoint_threshold_type: Method to determine semantic breakpoints \n",
    "                ('percentile' or 'standard_deviation').\n",
    "            breakpoint_threshold_amount: Threshold value for determining breakpoints \n",
    "                (higher = fewer chunks).\n",
    "            min_chunk_size: Minimum number of sentences per chunk.\n",
    "            max_chunk_size: Maximum number of sentences per chunk (not used in parent class).\n",
    "            overlap_sentences: Number of sentences to include before and after each chunk for context.\n",
    "        \"\"\"\n",
    "        # Initialize the parent SemanticChunker\n",
    "        # This provides the basic semantic chunking functionality\n",
    "        super().__init__(\n",
    "            embeddings=embeddings,\n",
    "            breakpoint_threshold_type=breakpoint_threshold_type,\n",
    "            breakpoint_threshold_amount=breakpoint_threshold_amount,\n",
    "            min_chunk_size=min_chunk_size,\n",
    "        )\n",
    "        # Store additional configuration parameters\n",
    "        self.max_chunk_size = max_chunk_size  # Maximum chunk size (for future implementation)\n",
    "        self.overlap_sentences = overlap_sentences  # How many sentences to overlap between chunks\n",
    "        self.gliner_model = gliner_model  # GLiNER model for entity recognition\n",
    "        # Entity types to extract with GLiNER\n",
    "        # This comprehensive list covers most entities in business emails\n",
    "        self.ner_labels = [\"date\", \"location\", \"person\", \"action\", \"finance\", \"legal\", \"event\", \"product\", \"organization\"]\n",
    "        self.splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,  # ~250â€“300 tokens worth of characters\n",
    "            chunk_overlap=0\n",
    "        )\n",
    "\n",
    "    def _split_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into sentences using regex.\n",
    "        \n",
    "        A utility method to break text into sentences based on common sentence\n",
    "        ending patterns (.!?). This is used both for chunking and overlap management.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to split into sentences\n",
    "            \n",
    "        Returns:\n",
    "            List of sentences extracted from the text\n",
    "        \"\"\"\n",
    "        # Basic sentence splitter using regex; can replace with spaCy or nltk if needed\n",
    "        # Splits on period, exclamation, or question mark followed by a space\n",
    "        # Also strips whitespace from each sentence and filters out empty strings\n",
    "        return [s.strip() for s in re.split(r'(?<=[.!?]) +', text) if s.strip()]\n",
    "\n",
    "    def _get_ner_spans(self, text: str) -> List[Dict[str, int]]:\n",
    "        \"\"\"Extract named entity spans and information from text.\n",
    "        \n",
    "        A core method for entity-aware chunking. It extracts entity locations and data\n",
    "        to prevent entities from being split across chunk boundaries.\n",
    "        \n",
    "        For shorter texts (<300 words), processes the entire text at once.\n",
    "        For longer texts, delegates to _get_ner_spans_long method.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to extract entities from\n",
    "            \n",
    "        Returns:\n",
    "            Tuple containing (entity_spans, entity_objects)\n",
    "        \"\"\"\n",
    "        chunks = self.splitter.split_text(text)\n",
    "        all_spans = []\n",
    "        all_entities = []\n",
    "\n",
    "        try:\n",
    "            for chunk in chunks:\n",
    "                # GLiNER model processes text and returns entity information\n",
    "                entities = self.gliner_model.predict_entities(chunk, self.ner_labels, threshold=0.5)\n",
    "                # Extract start/end character positions for each entity (for boundary adjustment)\n",
    "                spans = [(e['start'], e['end']) for e in entities if 'start' in e and 'end' in e]\n",
    "                all_spans.extend(spans)\n",
    "                all_entities.extend(entities)\n",
    "            return all_spans, all_entities\n",
    "        except Exception as e:\n",
    "            # Log any errors in entity extraction but continue processing\n",
    "            print(f\"NER extraction error: {e}\")\n",
    "            return [], []\n",
    "\n",
    "       \n",
    "    def _format_gliner_entities(self, entities: list) -> str:\n",
    "        \"\"\"Format extracted entities into a human-readable text description.\n",
    "        \n",
    "        Creates a natural language description of entities found in the text,\n",
    "        grouped by entity type, which can be used to enrich document content\n",
    "        or metadata for improved retrieval.\n",
    "        \n",
    "        Args:\n",
    "            entities: List of entity objects from GLiNER\n",
    "            \n",
    "        Returns:\n",
    "            Formatted entity description string\n",
    "        \"\"\"\n",
    "        if not entities:\n",
    "            return \"\"\n",
    "        \n",
    "        # Group entities by type to create more readable descriptions\n",
    "        grouped = defaultdict(list)\n",
    "        for ent in entities:\n",
    "            label = ent[\"label\"].lower()\n",
    "            text = ent[\"text\"].strip()\n",
    "            # Avoid duplicates within each entity type\n",
    "            if text not in grouped[label]:\n",
    "                grouped[label].append(text)\n",
    "\n",
    "        # Format each entity type into a natural language phrase\n",
    "        # This creates human-readable entity summaries for each type\n",
    "        phrases = []\n",
    "        for label, items in grouped.items():\n",
    "            readable_items = \", \".join(items)\n",
    "            # Format differently based on entity type for better readability\n",
    "            if label == \"person\":\n",
    "                phrases.append(f\"people mentioned include {readable_items}\")\n",
    "            elif label == \"date\":\n",
    "                phrases.append(f\"dates mentioned include {readable_items}\")\n",
    "            elif label == \"location\":\n",
    "                phrases.append(f\"locations mentioned include {readable_items}\")\n",
    "            elif label == \"finance\":\n",
    "                phrases.append(f\"financial terms include {readable_items}\")\n",
    "            elif label == \"organization\":\n",
    "                phrases.append(f\"organizations mentioned include {readable_items}\")\n",
    "            elif label == \"product\":\n",
    "                phrases.append(f\"products or services mentioned include {readable_items}\")\n",
    "            elif label == \"event\":\n",
    "                phrases.append(f\"events mentioned include {readable_items}\")\n",
    "            elif label == \"legal\":\n",
    "                phrases.append(f\"legal terms mentioned include {readable_items}\")\n",
    "            elif label == \"action\":\n",
    "                phrases.append(f\"actions or verbs include {readable_items}\")\n",
    "            else:\n",
    "                phrases.append(f\"{label}s mentioned include {readable_items}\")\n",
    "\n",
    "        # Combine all phrases into a single description\n",
    "        # This forms a comprehensive entity summary for the chunk\n",
    "        return \"This passage contains \" + \"; \".join(phrases) + \". \"\n",
    "\n",
    "\n",
    "    def _adjust_chunk_boundaries(self, text: str, chunks: List[str], spans: List[tuple]) -> List[str]:\n",
    "        \"\"\"Adjust chunk boundaries to prevent splitting entities.\n",
    "        \n",
    "        Ensures that named entities aren't split across chunks by extending\n",
    "        chunk boundaries to fully include any entity that would be split.\n",
    "        This is a key innovation in this chunker - preserving entity integrity.\n",
    "        \n",
    "        Args:\n",
    "            text: The full source text\n",
    "            chunks: List of initially determined chunks\n",
    "            spans: List of entity spans (start, end) to preserve\n",
    "            \n",
    "        Returns:\n",
    "            List of adjusted chunks with preserved entity boundaries\n",
    "        \"\"\"\n",
    "        adjusted_chunks = []\n",
    "        for chunk in chunks:\n",
    "            # Find the position of this chunk in the original text\n",
    "            start_idx = text.find(chunk)\n",
    "            end_idx = start_idx + len(chunk)\n",
    "\n",
    "            # Extend chunk boundaries to include any overlapping entity\n",
    "            # This ensures no entity is split across chunk boundaries\n",
    "            for ent_start, ent_end in spans:\n",
    "                # If an entity overlaps with this chunk boundary\n",
    "                if start_idx < ent_end and end_idx > ent_start:\n",
    "                    # Extend the chunk to fully include the entity\n",
    "                    start_idx = min(start_idx, ent_start)\n",
    "                    end_idx = max(end_idx, ent_end)\n",
    "\n",
    "            # Extract the adjusted chunk from the text\n",
    "            adjusted_chunk = text[start_idx:end_idx].strip()\n",
    "            adjusted_chunks.append(adjusted_chunk)\n",
    "\n",
    "        return adjusted_chunks\n",
    "\n",
    "    def create_documents(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        metadatas: Optional[List[Dict[str, Any]]] = None\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Create LangChain Document objects from texts with enhanced chunking.\n",
    "        \n",
    "        This method:\n",
    "        1. Splits texts into semantic chunks\n",
    "        2. Adjusts chunk boundaries to preserve entities\n",
    "        3. Adds sentence overlap for context continuity\n",
    "        4. Enriches metadata with entity information\n",
    "        5. Returns Document objects ready for vectorization\n",
    "        \n",
    "        Args:\n",
    "            texts: List of input texts to process\n",
    "            metadatas: Optional list of metadata dictionaries for each text\n",
    "            \n",
    "        Returns:\n",
    "            List of LangChain Document objects with enhanced content and metadata\n",
    "        \"\"\"\n",
    "        # Initialize empty metadata if none provided\n",
    "        if metadatas is None:\n",
    "            metadatas = [{} for _ in texts]\n",
    "\n",
    "        all_docs = []\n",
    "\n",
    "        # Process each text with its corresponding metadata\n",
    "        for i, (text, metadata) in enumerate(zip(texts, metadatas)):\n",
    "            # Split text into sentences\n",
    "            sentences = self._split_sentences(text)\n",
    "            # Create mapping from sentence to its index for quick lookup\n",
    "            sentence_to_idx = {s: idx for idx, s in enumerate(sentences)}\n",
    "            \n",
    "            # Extract named entities\n",
    "            spans, _ = self._get_ner_spans(text)\n",
    "            \n",
    "            # Get initial semantic chunks using parent class method\n",
    "            raw_chunks = self.split_text(text)\n",
    "            \n",
    "            # Adjust chunk boundaries to preserve entity mentions\n",
    "            adjusted_chunks = self._adjust_chunk_boundaries(text, raw_chunks, spans)\n",
    "\n",
    "            # Process each adjusted chunk\n",
    "            for chunk in adjusted_chunks:\n",
    "                # Split the chunk into sentences for overlap processing\n",
    "                chunk_sentences = self._split_sentences(chunk)\n",
    "\n",
    "                # Skip empty chunks\n",
    "                if not chunk_sentences:\n",
    "                    continue\n",
    "\n",
    "                # Find original sentence indices for this chunk\n",
    "                first_sentence = chunk_sentences[0]\n",
    "                last_sentence = chunk_sentences[-1]\n",
    "                start_idx = sentence_to_idx.get(first_sentence, 0)\n",
    "                end_idx = sentence_to_idx.get(last_sentence, start_idx)\n",
    "\n",
    "                # Add overlap sentences before and after\n",
    "                # This creates continuity between chunks\n",
    "                prefix = sentences[max(0, start_idx - self.overlap_sentences):start_idx]\n",
    "                suffix = sentences[end_idx + 1:end_idx + 1 + self.overlap_sentences]\n",
    "\n",
    "                # Combine into final chunk with overlap\n",
    "                full_chunk = \" \".join(prefix + chunk_sentences + suffix).strip()\n",
    "                \n",
    "                # Add entity information to metadata\n",
    "                # This enriches the chunk with structured entity data\n",
    "                _, chunk_entities = self._get_ner_spans(full_chunk)\n",
    "                metadata[\"entities\"] = self._format_gliner_entities(chunk_entities)\n",
    "                \n",
    "                \n",
    "                # Create LangChain Document with prefix, enhanced content and metadata\n",
    "                all_docs.append(Document(\n",
    "                    page_content=\"passage: \" + full_chunk,\n",
    "                    metadata=metadata\n",
    "                ))\n",
    "        \n",
    "        # --- Deduplication Step ---\n",
    "        unique_docs_dict = {}\n",
    "        for doc in all_docs:   \n",
    "            # Use page_content as the key for uniqueness check\n",
    "            if doc.page_content not in unique_docs_dict:\n",
    "                unique_docs_dict[doc.page_content] = doc\n",
    "\n",
    "        # Convert the dictionary values back to a list of unique documents\n",
    "        all_docs = list(unique_docs_dict.values())\n",
    "        # ------------------------\n",
    "\n",
    "        return all_docs\n",
    "\n",
    "    def split_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"Split existing LangChain Documents into smaller chunks.\n",
    "        \n",
    "        This is a convenience method for processing documents that are\n",
    "        already in LangChain Document format.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of Documents to split\n",
    "            \n",
    "        Returns:\n",
    "            List of split Documents with enhanced features\n",
    "        \"\"\"\n",
    "        # Extract text and metadata from documents\n",
    "        texts = [doc.page_content for doc in documents]\n",
    "        metadatas = [doc.metadata for doc in documents]\n",
    "        # Delegate to create_documents method\n",
    "        return self.create_documents(texts, metadatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b31d53dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_texts = df[\"message\"].iloc[2001:5000].dropna().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5576886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean text while preserving useful characters:\n",
    "    - Removes weird/unprintable symbols\n",
    "    - Keeps letters, numbers, basic punctuation: @ . , ? : ; ! _ ( ) &\n",
    "    - Normalizes whitespace\n",
    "    \"\"\"\n",
    "    # Remove anything not in the allowed set\n",
    "    text = re.sub(r\"[^A-Za-z0-9@.,?;:!()&\\_ ]\", '', text)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c95cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "import torch\n",
    "\n",
    "# Use Microsoft E5 model instead of MPNet\n",
    "model_kwargs = {'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}  # does l2 norm for the cos sim\n",
    "modelemb = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/e5-base-v2\", \n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dcfa5c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from gliner import GLiNER\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "import re\n",
    "\n",
    "# ============ Setup Models ============\n",
    "\n",
    "# Load GLiNER for NER\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gliner_model = GLiNER.from_pretrained(\n",
    "    \"glinermodellocal\",\n",
    "    local_files_only=True\n",
    ")\n",
    "gliner_model.config.max_len = 512\n",
    "gliner_model.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "# ============ Configuration ============\n",
    "labels = [\"date\", \"location\", \"person\", \"action\", \"finance\", \"legal\", \"event\", \"product\", \"organization\"]\n",
    "\n",
    "# ============ Helper Functions ============\n",
    "from dateutil import parser\n",
    "\n",
    "def parse_email_date(date_tokens: List[str]) -> str:\n",
    "    raw_date_str = \" \".join(date_tokens)\n",
    "    try:\n",
    "        parsed = parser.parse(raw_date_str, fuzzy=True)\n",
    "        return parsed.strftime(\"%m-%d-%Y\")\n",
    "    except Exception as e:\n",
    "        print(f\"Date parse error: {e}\")\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "def extract_email_metadata(msg, df_idx):\n",
    "    split_msg = msg.split()\n",
    "    metadata = {}\n",
    "    metadata[\"Message-ID\"] = split_msg[split_msg.index(\"Message-ID:\")+1]\n",
    "    metadata[\"filename\"] = df[\"file\"].iloc[df_idx]\n",
    "    try:\n",
    "        metadata['sender'] = split_msg[split_msg.index(\"From:\") + 1]\n",
    "        recips = []\n",
    "        try:\n",
    "            for idx in range(split_msg.index(\"To:\") + 1, split_msg.index(\"Subject:\")):\n",
    "                recips.append(split_msg[idx])\n",
    "        except:\n",
    "            for idx in range(split_msg.index(\"X-To:\") + 1, split_msg.index(\"Subject:\")):\n",
    "                recips.append(split_msg[idx])\n",
    "        metadata['recipient'] = \" \".join(recips)\n",
    "        metadata['date'] = parse_email_date(split_msg[split_msg.index(\"Date:\") + 1: split_msg.index(\"Date:\") + 7])\n",
    "        metadata['subject'] = \" \".join(split_msg[split_msg.index(\"Subject:\") + 1:split_msg.index(\"Mime-Version:\")])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Metadata extraction error:\", e)\n",
    "    return metadata, split_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9ea313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import random\n",
    "msg = df['message'][random.randint(0,500)]\n",
    "print(msg.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6e2d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import random\n",
    "idx = random.randint(501,1999)\n",
    "msg = df['message'][idx]\n",
    "print(msg)\n",
    "print(clean_text(msg))\n",
    "\n",
    "# Example usage of our new EnhancedSemanticChunker\n",
    "enhanced_chunker = EnhancedSemanticChunker(\n",
    "    embeddings=modelemb,\n",
    "    breakpoint_threshold_type=\"percentile\",\n",
    "    breakpoint_threshold_amount=50,  \n",
    "    min_chunk_size=5,\n",
    "    overlap_sentences=2,  \n",
    "    gliner_model=gliner_model\n",
    ")\n",
    "\n",
    "\n",
    "# Test the enhanced chunker on a sample email\n",
    "sample_email = msg\n",
    "metadata, split_msg = extract_email_metadata(sample_email, idx)\n",
    "msg_start = split_msg.index(\"X-FileName:\")\n",
    "full_content = clean_text(\" \".join(split_msg[msg_start + 2:]))\n",
    "# print(full_content)\n",
    "# Create document with the enhanced chunker\n",
    "documents = enhanced_chunker.create_documents(\n",
    "    texts=[full_content],\n",
    "    metadatas=[metadata]\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Number of chunks after deduplication: {len(documents)}\")\n",
    "\n",
    "# Now use 'deduplicated_documents' for further processing (indexing, etc.)\n",
    "for i, doc in enumerate(documents):  \n",
    "    print(f\"\\n--- Unique Chunk {i+1}/{len(documents)} ---\")\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebb2e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(email_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbc2dd47",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "import pprint\n",
    "\n",
    "\n",
    "def create_docslist():\n",
    "    docslist = []\n",
    "    for i, sample_email in enumerate(email_texts):\n",
    "        print(f\"Processing email {i+1}/{len(email_texts)}\")\n",
    "        metadata, split_msg = extract_email_metadata(sample_email, i)\n",
    "        msg_start = split_msg.index(\"X-FileName:\")\n",
    "        full_content = clean_text(\" \".join(split_msg[msg_start + 3:]))\n",
    "        # print(full_content)\n",
    "        # Create document with the enhanced chunker\n",
    "        documents = enhanced_chunker.create_documents(\n",
    "            texts=[full_content],\n",
    "            metadatas=[metadata]\n",
    "        )\n",
    "\n",
    "        print(f\"Created {len(documents)} enhanced chunks\")\n",
    "        for i, doc in enumerate(documents):  \n",
    "            print(f\"\\n--- Chunk {i+1}/{len(documents)} ---\")\n",
    "            print(f\"Content: {doc.page_content}\")\n",
    "            print(f\"Metadata: {doc.metadata}\")\n",
    "        docslist.extend(documents)\n",
    "    return docslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "480e5774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def l2_normalize(embeddings: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    L2-normalizes an array.\n",
    "    If the input is 1D, normalize the whole vector.\n",
    "    If it's 2D, normalize each row.\n",
    "    \"\"\"\n",
    "    if embeddings.ndim == 1:\n",
    "        norm = np.linalg.norm(embeddings)\n",
    "        return embeddings / norm if norm != 0 else embeddings\n",
    "    else:\n",
    "        norm = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        norm[norm == 0] = 1  # avoid division by zero\n",
    "        return embeddings / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc059d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "# docslist2 = create_docslist()\n",
    "# docslist =  joblib.load(\"docslist.pkl\")\n",
    "# docslist.extend(docslist2)\n",
    "# joblib.dump(docslist,\"docslist.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61fa5b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_by_message_id(df, message_id):\n",
    "    return df[df['message'].str.contains(f\"Message-ID: <{message_id}>\", na=False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20460631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract text from Document objects\n",
    "# docs_texts = [doc.page_content for doc in docslist]  # Get only text content\n",
    "\n",
    "# # Generate embeddings for each text\n",
    "# embeddings = modelemb.embed_documents(docs_texts)  # List[List[float]]\n",
    "\n",
    "# embeddings = l2_normalize(np.array(embeddings))  # Ensure it's NumPy and normalized\n",
    "\n",
    "# np.save(\"embeddings.npy\",embeddings)\n",
    "# # Create (text, embedding) pairs for FAISS\n",
    "# text_embedding_pairs = list(zip(docs_texts, embeddings))  # Convert np.array to list\n",
    "\n",
    "from langchain_qdrant import Qdrant\n",
    "\n",
    "# qdrant = Qdrant.from_documents(\n",
    "#     docslist,\n",
    "#     modelemb,\n",
    "#     path=\"qdrant_db\",\n",
    "#     collection_name=\"my_documents\",\n",
    "# )\n",
    "qdrant = Qdrant.from_existing_collection(modelemb,\"qdrant_db\",VECTOR_DB_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b8b9600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db = FAISS.load_local(\"email_faiss_normalized_e5_enhanced\",modelemb,allow_dangerous_deserialization=True)\n",
    "db = qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65a07681",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modelemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af619588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from initialize_groq import init_groq\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "            Answer question based on EMAIL METADATA AND CONTENT provided. CITE YOUR SOURCES.\n",
    "            {context}\n",
    "\n",
    "            Here is question:\n",
    "            {input}\n",
    "        \"\"\"\n",
    ")\n",
    "\n",
    "document_prompt = PromptTemplate.from_template(\n",
    "    \"METADATA: Source: {sender}\\nDate: {date}\\n Recipients: {recipient}\\nSubject: {subject}\\nEntities: {entities}\\n\\nContent: {page_content}\"\n",
    ")\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={'k':20, 'search_type':'mmr','lambda_mult':0.2})\n",
    "\n",
    "_, llm = init_groq(model_name=\"llama-3.3-70b-versatile\")\n",
    "import random\n",
    "document_chain = create_stuff_documents_chain(llm, prompt=prompt, document_prompt=document_prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# Retrieve Top-K Similar Documents (Initial Broad Search)\n",
    "# retriever_topk = db.as_retriever(search_kwargs={'k': 20,'fetch_k' : 100, 'search_type': 'similarity_s core_threshold','score_threshold':0.75})  # Retrieve more docs first\n",
    "retriever_topk = db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'score_threshold':0.65,'k':10})\n",
    "# MMR for Diversity (Reduce Redundant Docs)\n",
    "retriever_mmr = db.as_retriever(search_type=\"mmr\", search_kwargs={'k':10,'lambda_mult': 1})  \n",
    "\n",
    "# Create the Hybrid Retrieval Pipeline\n",
    "retrieval_chain_topk = create_retrieval_chain(retriever_topk, document_chain)  # Initial broad search\n",
    "retrieval_chain_mmr = create_retrieval_chain(retriever_mmr, document_chain)    # Apply MMR re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4d5cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "# \"What does randy need to send a schedule of?\",\n",
    "#     \"What are some of randy's action items?\",\n",
    "#     \"What is Philip's proposal focused on, and can you provided details about the proposal?\",\n",
    "#     \"Can you provide me more detail about the microturbine power generation deal?\"\n",
    "query = \"query: is MSEB an indian company? 402 crore amount? its relation to enron?\"\n",
    "# pprint.pprint(retrieval_chain_topk.invoke({\"input\":query}))\n",
    "pprint.pprint(retrieval_chain_mmr.invoke({\"input\":query}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18434201",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "import numpy as np\n",
    "test_questions = [\n",
    "    # \"What does randy need to send a schedule of?\",\n",
    "    # \"What are some of randy's action items?\",\n",
    "    # \"What is Philip's proposal focused on, and can you provided details about the proposal?\",\n",
    "    # \"Can you provide me more detail about the microturbine power generation deal?\",\n",
    "    # \"What needs to be faxed?\"\n",
    "    \"Are there hints of a scandal in the emails?\",\n",
    "    \"What did jeffrey skilling tell john arnold\"\n",
    "]\n",
    "for text in test_questions:\n",
    "    # Define query\n",
    "    query = \"query: \" + text\n",
    "    pprint.pprint(retrieval_chain_mmr.invoke({\"input\":query}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50114563",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "import numpy as np\n",
    "test_questions = [\n",
    "    \"query: What does randy need to send a schedule of?\",\n",
    "    \"query: What are some of randy's action items?\",\n",
    "    \"query: What is Philip's proposal focused on, and can you provided details about the proposal?\",\n",
    "    \"query: Can you provide me more detail about the microturbine power generation deal?\",\n",
    "    \"query: What needs to be faxed?\"\n",
    "]\n",
    "for text in test_questions:\n",
    "    print(\"=========================================================\")\n",
    "    query = \"query: \" + text\n",
    "    query_embedding = np.array(model.embed_query(query))\n",
    "    # query_embedding = l2_normalize(query_embedding)  \n",
    "    topk_results = db.similarity_search_with_score_by_vector(\n",
    "        embedding=query_embedding.tolist(),  # List[float]\n",
    "        k=5\n",
    "    )\n",
    "\n",
    "    mmr_results = db.max_marginal_relevance_search_with_score_by_vector(\n",
    "        embedding=query_embedding.tolist(),  # List[float]\n",
    "        k=5,\n",
    "        lambda_mult=0.8         \n",
    "    )\n",
    "\n",
    "    # Sort by L2 distance (ascending: lower = more similar)\n",
    "    topk_sorted = sorted(topk_results, key=lambda x: x[1])\n",
    "    \n",
    "    mmr_sorted = sorted(mmr_results, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Display results with L2 distance and cosine similarity\n",
    "    for doc, mmr_score in mmr_sorted:\n",
    "        # docembedding = l2_normalize(np.array(modelemb.embed_documents([doc.page_content])))\n",
    "        # cos_sim = float(np.dot(query_embedding, docembedding.reshape(-1)))\n",
    "        pprint.pprint(f\"Document:\\n {doc.page_content} | MMR Score: {mmr_score:.4f}\")\n",
    "        \n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "    for doc, l2_score in topk_sorted:\n",
    "        # Convert L2 distance to cosine similarity (assuming L2-normalized)\n",
    "        cosine_sim = 1 - (l2_score ** 2) / 2\n",
    "        pprint.pprint(f\"Document: {doc.page_content[:100]} | L2 Distance: {l2_score:.4f} | Cosine Sim: {cosine_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43a0e646",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "# Ensure retrieval_chain is correctly defined before calling this tool\n",
    "@tool\n",
    "def ragtool(query: str, num_docs: int) -> str:\n",
    "    \"\"\"\n",
    "    This is a retrieval-augmented generation (RAG) tool that queries a vector store \n",
    "    containing Enron emails.\n",
    "    \n",
    "    Parameters:\n",
    "    query (str): The input query for retrieval.\n",
    "    num_docs (int): The number of documents to retrieve.\n",
    "    Returns:\n",
    "    str: The retrieved answer from the vector store.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        answer = retrieval_chain_topk.invoke({\"input\": query})['answer']\n",
    "        return f\"Here is the ANSWER. \\n ```{answer}```\\n DO NOT USE THE TOOL REPEATEDLY. SHOW THE ANSWER TO THE USER. \\n\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: Failed to retrieve answer. Details: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97aa9092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "\n",
    "\n",
    "toolnode = ToolNode([ragtool])\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    state[\"messages\"]\n",
    "    messages = state[\"messages\"]\n",
    "    #print(messages)\n",
    "    llm.groq_api_key = random.choice(api_keys)\n",
    "    llm_with_tool = llm.bind_tools([ragtool])\n",
    "    response = llm_with_tool.invoke(messages)\n",
    "    \n",
    "    \n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "from langgraph.graph import END\n",
    "def router_function(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "memory = MemorySaver()\n",
    "workflow = StateGraph(MessagesState)    \n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(toolnode)\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    router_function,\n",
    "    {\n",
    "       \"tools\": \"tools\",\n",
    "       END: END,\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a73210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_png\n",
    "display_png(app.get_graph().draw_mermaid_png(),raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9743d10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "while True:\n",
    "    theinput = input(\"Enter something: \")\n",
    "    if 'exit' in theinput:\n",
    "        break\n",
    "    inp = {\"messages\":[theinput]}\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": 1}}\n",
    "    events = app.stream(inp, config=config, stream_mode=\"values\")\n",
    "\n",
    "    for event in events:\n",
    "        event[\"messages\"][-1].pretty_print()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7acb9f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced query handling with multi-query generation\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Define the multi-query prompt template\n",
    "# This template instructs the LLM to generate multiple search queries from a single user question\n",
    "multi_template = \"\"\"You are an expert at querying search engines. You specialize in understanding natural language queries and generating multiple search\n",
    "queries that, taken together, would help provide a comprehensive answer to the user's question.\n",
    "\n",
    "Main Question: {question}\n",
    "\n",
    "Let's break this down. Generate 4 search queries for querying a knowledge store about emails. \n",
    "Make sure these queries use language that would appear in actual emails.\n",
    "Remember to keep them short, using keywords that would be found in emails.\n",
    "Keep them straightforward and distinct from each other.\n",
    "Formulate them from different angles to solve the main query.\n",
    "\n",
    "Return a bullet list with â€¢ at the start of each question:\n",
    "\n",
    "â€¢ query 1\n",
    "â€¢ query 2\n",
    "â€¢ etc.\n",
    "\"\"\"\n",
    "\n",
    "# Create a processor to generate multiple search queries from a single question\n",
    "multi_query_prompt = PromptTemplate.from_template(multi_template)\n",
    "multi_query_chain = multi_query_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0ecdef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to run multiple queries and combine results for better coverage\n",
    "import time\n",
    "\n",
    "def run_multi_query(main_query, query_generator=multi_query_chain, single_query_chain=retrieval_chain_topk):\n",
    "    \"\"\"\n",
    "    Run a multi-query retrieval process to improve search results accuracy.\n",
    "    \n",
    "    This function:\n",
    "    1. Takes a user query and generates multiple search queries using the LLM\n",
    "    2. Executes each generated query against the retrieval system\n",
    "    3. Combines and summarizes the results for a comprehensive answer\n",
    "    \n",
    "    Args:\n",
    "        main_query: The original user question\n",
    "        query_generator: Chain to generate multiple search queries\n",
    "        single_query_chain: Chain to execute individual queries\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with consolidated results and performance metrics\n",
    "    \"\"\"\n",
    "    # Start timing the process\n",
    "    start_time = total_start_time = time.time()\n",
    "    \n",
    "    # Generate multiple search queries from the main question\n",
    "    result = query_generator.invoke({\"question\": main_query})\n",
    "    \n",
    "    # Extract the generated queries from the bullet point list\n",
    "    sub_questions = [q.strip() for q in result.split('â€¢') if q.strip()]\n",
    "    \n",
    "    # Record query generation time\n",
    "    gen_time = time.time() - start_time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Track all retrieved documents and their sources\n",
    "    all_docs = []\n",
    "    all_results = []\n",
    "    \n",
    "    # Process each generated query\n",
    "    print(\"Generated Questions:\")\n",
    "    for i, question in enumerate(sub_questions):\n",
    "        print(f\"{i+1}. {question}\")\n",
    "        # Execute the query against the retrieval system\n",
    "        chain_result = single_query_chain.invoke({\"input\": question})\n",
    "        all_results.append(chain_result)\n",
    "        \n",
    "        # Track the documents retrieved for this query\n",
    "        if \"context\" in chain_result:\n",
    "            all_docs.extend(chain_result[\"context\"])\n",
    "    \n",
    "    # Record search execution time\n",
    "    search_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate combined result using the original query\n",
    "    # This ensures the answer is based on all retrieved information\n",
    "    final_answer = single_query_chain.invoke({\n",
    "        \"input\": main_query,\n",
    "        \"context\": all_docs[:10]  # Limit to top 10 most relevant documents\n",
    "    })\n",
    "    \n",
    "    # Record total processing time\n",
    "    total_time = time.time() - total_start_time\n",
    "    \n",
    "    # Return comprehensive results with timing metrics\n",
    "    return {\n",
    "        \"main_query\": main_query,\n",
    "        \"generated_queries\": sub_questions,\n",
    "        \"individual_answers\": all_results,\n",
    "        \"final_answer\": final_answer[\"answer\"],\n",
    "        \"all_docs\": all_docs,\n",
    "        \"timing\": {\n",
    "            \"query_generation\": gen_time,\n",
    "            \"search_execution\": search_time,\n",
    "            \"total_processing\": total_time\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e4fac2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test queries for evaluating retrieval performance\n",
    "test_queries = [\n",
    "    \"What do we know about Skilling's involvement in Enron's financial reporting?\",\n",
    "    \"What are the main topics discussed in emails from Kenneth Lay?\",\n",
    "    \"How did Enron executives discuss the California energy crisis in their emails?\",\n",
    "    \"What discussions were happening about LJM partnerships in the months before Enron's collapse?\",\n",
    "    \"What was discussed about mark-to-market accounting in emails?\",\n",
    "    \"Who was responsible for overseeing Special Purpose Entities at Enron?\",\n",
    "    \"What communication happened regarding Raptor structures?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf27ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage - process a test query with advanced retrieval\n",
    "result = run_multi_query(test_queries[0])\n",
    "# Print the final answer\n",
    "print(result[\"final_answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "205b7e3c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define tools for integration with LLMs and agent frameworks\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Create a RAG tool for Enron email queries - allows LLM to retrieve context\n",
    "@tool\n",
    "def ragtool(query: str, num_docs: int) -> str:\n",
    "    \"\"\"\n",
    "    This is a retrieval-augmented generation (RAG) tool that queries a vector store \n",
    "    containing Enron emails.\n",
    "    \n",
    "    Parameters:\n",
    "    query (str): The input query for retrieval.\n",
    "    num_docs (int): The number of documents to retrieve.\n",
    "    Returns:\n",
    "    str: The retrieved answer from the vector store.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        answer = retrieval_chain_topk.invoke({\"input\": query})['answer']\n",
    "        return f\"Here is the ANSWER. \\n ```{answer}```\\n DO NOT USE THE TOOL REPEATEDLY. SHOW THE ANSWER TO THE USER. \\n\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: Failed to retrieve answer. Details: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c493245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup agent framework with LangGraph for interactive email analysis\n",
    "from typing import Literal\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "# Create a tool node for the agent to use\n",
    "toolnode = ToolNode([ragtool])\n",
    "\n",
    "# Define the agent's reasoning function\n",
    "def call_model(state: MessagesState):\n",
    "    state[\"messages\"]\n",
    "    messages = state[\"messages\"]\n",
    "    #print(messages)\n",
    "    llm.groq_api_key = random.choice(api_keys)\n",
    "    llm_with_tool = llm.bind_tools([ragtool])\n",
    "    response = llm_with_tool.invoke(messages)\n",
    "    \n",
    "    \n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Define a router function to handle tool calls\n",
    "from langgraph.graph import END\n",
    "def router_function(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "# Set up the agent workflow graph\n",
    "memory = MemorySaver()\n",
    "workflow = StateGraph(MessagesState)    \n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(toolnode)\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    router_function,\n",
    "    {\n",
    "       \"tools\": \"tools\",\n",
    "       END: END,\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c7784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate visualization of the agent workflow\n",
    "from IPython.display import display_png\n",
    "display_png(app.get_graph().draw_mermaid_png(),raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31fb799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive chat loop for querying the email database\n",
    "import time\n",
    "while True:\n",
    "    theinput = input(\"Enter something: \")\n",
    "    if 'exit' in theinput:\n",
    "        break\n",
    "    inp = {\"messages\":[theinput]}\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": 1}}\n",
    "    events = app.stream(inp, config=config, stream_mode=\"values\")\n",
    "\n",
    "    for event in events:\n",
    "        event[\"messages\"][-1].pretty_print()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0f70a4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# The following code is an alternative agent design that was commented out\n",
    "# It shows a more complex workflow with filtering tools and summarization\n",
    "# Keeping as reference for potential future implementation\n",
    "# from typing import Literal, List\n",
    "# from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "# from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "# from langgraph.checkpoint.memory import MemorySaver\n",
    "# from langchain.memory import ChatMessageHistory\n",
    "# from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\n",
    "# from langgraph.prebuilt import ToolNode\n",
    "# from langchain_core.documents import Document\n",
    "# from langchain_core.tools import tool\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# # === Define Custom Tools ===\n",
    "\n",
    "\n",
    "# @tool\n",
    "# def filter_emails_by_keyword(emails: List[Document], keywords: List[str]) -> List[Document]:\n",
    "#     \"\"\"Filter emails that contain the given keywords in the content or metadata.\"\"\"\n",
    "#     def filter_email(email):\n",
    "#         content = email.page_content.lower()\n",
    "#         metadata = \" \".join(str(val).lower() for val in email.metadata.values())\n",
    "#         return any(keyword.lower() in content or keyword.lower() in metadata for keyword in keywords)\n",
    "    \n",
    "#     with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#         filtered_emails = list(filter(None, executor.map(lambda e: e if filter_email(e) else None, emails)))\n",
    "\n",
    "#     return filtered_emails\n",
    "\n",
    "# @tool\n",
    "# def filter_emails_by_metadata(\n",
    "#     emails: List[Document], sender: str = None, recipient: str = None, date: str = None\n",
    "# ) -> List[Document]:\n",
    "#     \"\"\"Filter emails by metadata fields like sender, recipient, or date.\"\"\"\n",
    "#     def filter_email(email):\n",
    "#         if sender and email.metadata.get(\"sender\", \"\").lower() != sender.lower():\n",
    "#             return None\n",
    "#         if recipient and recipient.lower() not in email.metadata.get(\"recipient\", \"\").lower():\n",
    "#             return None\n",
    "#         if date and date not in email.metadata.get(\"date\", \"\"):\n",
    "#             return None\n",
    "#         return email\n",
    "    \n",
    "#     with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#         filtered_emails = list(filter(None, executor.map(filter_email, emails)))\n",
    "    \n",
    "#     return filtered_emails\n",
    "\n",
    "# @tool\n",
    "# def summarize_emails(emails: List[Document]) -> List[Document]:\n",
    "#     \"\"\"Summarize emails before adding them to FAISS.\"\"\"\n",
    "#     _,llm = init_groq(model_name=\"llama-3.3-70b-versatile\")\n",
    "#     prompt = ChatPromptTemplate.from_template(\"Summarize the following email:\\n{email}\")\n",
    "#     summarized_docs = []\n",
    "#     for email in emails:\n",
    "#         chain = prompt | llm\n",
    "#         summary = chain.invoke(email.page_content)\n",
    "#         summarized_docs.append(Document(page_content=summary.content, metadata=email.metadata))\n",
    "#     return summarized_docs\n",
    "\n",
    "# toolnode = ToolNode([ragtool, filter_emails_by_keyword, filter_emails_by_metadata, summarize_emails])\n",
    "# llm_with_tool = llm.bind_tools([ragtool, filter_emails_by_keyword, filter_emails_by_metadata, summarize_emails])\n",
    "\n",
    "# # === Define Model Function ===\n",
    "# def call_model(state: MessagesState):\n",
    "#     \"\"\"Modify agent behavior to apply filtering and summarization before RAG.\"\"\"\n",
    "#     messages = state[\"messages\"]\n",
    "#     query = messages[-1]\n",
    "\n",
    "#     # Step 1: Apply Keyword Filtering\n",
    "#     filtered_emails = filter_emails_by_keyword.invoke({\"emails\": docslist, \"keywords\": [query.content]})\n",
    "\n",
    "#     # Step 2: Apply Metadata Filtering\n",
    "#     filtered_emails = filter_emails_by_metadata.invoke(\n",
    "#         {\"emails\": filtered_emails, \"sender\": \"\", \"recipient\": \"\", \"date\": \"\"}\n",
    "#     )\n",
    "\n",
    "#     # Step 3: Summarize Emails if Needed\n",
    "#     summarized_emails = summarize_emails.invoke({\"emails\": filtered_emails})\n",
    "\n",
    "#     # Step 4: Run RAG Tool on Filtered Emails\n",
    "#     state[\"messages\"].append(\"\\n\")\n",
    "#     response = llm_with_tool.invoke([summarized_emails])\n",
    "\n",
    "#     return {\"messages\": [response]}\n",
    "\n",
    "# # === Define Router Function ===\n",
    "# def router_function(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "#     messages = state[\"messages\"]\n",
    "#     last_message = messages[-1]\n",
    "#     if last_message.tool_calls:\n",
    "#         return \"tools\"\n",
    "#     return END\n",
    "\n",
    "# # === Build LangGraph Workflow ===\n",
    "# memory = MemorySaver()\n",
    "# workflow = StateGraph(MessagesState)\n",
    "# workflow.add_node(\"agent\", call_model)\n",
    "# workflow.add_node(toolnode)\n",
    "# workflow.add_edge(START, \"agent\")\n",
    "# workflow.add_conditional_edges(\n",
    "#     \"agent\",\n",
    "#     router_function,\n",
    "#     {\n",
    "#         \"tools\": \"tools\",\n",
    "#         END: END,\n",
    "#     },\n",
    "# )\n",
    "# workflow.add_edge(\"tools\", \"agent\")\n",
    "# app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# while True:\n",
    "#     theinput = input(\"Enter something: \")\n",
    "#     if 'exit' in theinput:\n",
    "#         break\n",
    "#     inp = {\"messages\":[theinput]}\n",
    "\n",
    "#     config = {\"configurable\": {\"thread_id\": 1}}\n",
    "#     events = app.stream(inp, config=config, stream_mode=\"values\")\n",
    "\n",
    "#     for event in events:\n",
    "#         event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "TheVirtualEnv",
   "language": "python",
   "name": "thevirtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
